"""FlashVSR æ¨ç†æœåŠ¡å°è£…."""

from __future__ import annotations

import inspect
import os
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from threading import Lock
from typing import Any, Callable, Optional

# é¿å… PyTorch é¢„ç•™çš„å¤§å—æ˜¾å­˜æ— æ³•å¤ç”¨ï¼Œé»˜è®¤å¯ç”¨å¯æ‰©å±•åˆ†æ®µåˆ†é…ã€‚
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

import torch

from app.config import settings
from app.flashvsr_core.diffsynth.configs.model_config import (
    FLASHVSR_TINY_LONG_BASE_FILES,
    FLASHVSR_TINY_LONG_EXTRA_FILES,
    FLASHVSR_TINY_LONG_PROMPT_FILE,
    FLASHVSR_TINY_LONG_REPO_ID,
)
from app.services.chunk_export import build_chunk_base_name
from app.services.flashvsr_device import (
    resolve_device,
    decide_cache_offload_device,
    parse_pipeline_parallel,
)
from app.services.flashvsr_io import (
    prepare_input,
    merge_video_chunks,
)
from app.services.video_export import VideoExporter
from app.services.progress import ProgressReporter
from app.services.aspect_strategy import (
    AspectStrategy,
    CenterCrop128Strategy,
    PadThenCropStrategy,
)

# Block-Sparse æ³¨æ„åŠ›ä¾èµ–çš„ CUDA æ‰©å±•è·¯å¾„ï¼ˆå®é™…å¯¼å…¥åœ¨ FlashVSR pipeline åˆå§‹åŒ–æ—¶è§¦å‘ï¼‰
BLOCK_SPARSE_PATH = settings.THIRD_PARTY_BLOCK_SPARSE_PATH
if str(BLOCK_SPARSE_PATH) not in sys.path:
    sys.path.insert(0, str(BLOCK_SPARSE_PATH))


@dataclass
class PipelineHandle:
    """ç¼“å­˜çš„ Pipeline å®ä¾‹ä¿¡æ¯."""

    variant: str
    pipeline: Any
    device: str
    default_kwargs: dict[str, Any]


@dataclass
class FlashVSRJobParams:
    """å°è£…ä¸€æ¬¡ FlashVSR æ¨ç†ä½œä¸šçš„å‚æ•°."""

    scale: float = 4.0
    sparse_ratio: float = 2.0
    local_range: int = 11
    seed: int = 0
    model_variant: str = settings.DEFAULT_MODEL_VARIANT
    audio_path: Optional[str] = None
    preserve_aspect_ratio: bool = False
    source_width: Optional[int] = None
    source_height: Optional[int] = None


class FlashVSRPipelineManager:
    """ç®¡ç† FlashVSR pipeline çš„ç”Ÿå‘½å‘¨æœŸä¸ç¼“å­˜."""

    SUPPORTED_VARIANTS: tuple[str, ...] = ("tiny_long",)
    BASE_MODEL_FILES: tuple[str, ...] = FLASHVSR_TINY_LONG_BASE_FILES
    FULL_ONLY_FILES: tuple[str, ...] = FLASHVSR_TINY_LONG_EXTRA_FILES
    PROMPT_TENSOR_FILE = settings.FLASHVSR_PROMPT_TENSOR_PATH

    def __init__(self) -> None:
        self._pipelines: dict[str, PipelineHandle] = {}
        self._lock: Lock = Lock()
        self._auto_download_used: bool = False
        self._auto_download_source: Optional[str] = None

    def inspect_assets(self) -> dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹æƒé‡æƒ…å†µï¼Œä¾›ç³»ç»ŸçŠ¶æ€å’Œè¯Šæ–­ä½¿ç”¨."""

        model_path = settings.FLASHVSR_MODEL_PATH
        file_status: dict[str, bool] = {}

        for filename in self.BASE_MODEL_FILES + self.FULL_ONLY_FILES:
            file_status[filename] = (model_path / filename).exists()
        file_status[FLASHVSR_TINY_LONG_PROMPT_FILE] = self.PROMPT_TENSOR_FILE.exists()

        def _ready(extra: tuple[str, ...] = ()) -> bool:
            base_ready = file_status[FLASHVSR_TINY_LONG_PROMPT_FILE] and all(
                file_status[name] for name in self.BASE_MODEL_FILES
            )
            extra_ready = all(file_status[name] for name in extra)
            return base_ready and extra_ready

        ready_variants = {
            "tiny_long": _ready(),
        }
        missing_files = [name for name, ok in file_status.items() if not ok]

        if self._auto_download_used:
            model_source = self._auto_download_source or "ModelScope"
            auto_download_used = True
        else:
            auto_download_used = False
            if model_path.exists() and not missing_files:
                model_source = "local"
            else:
                model_source = None

        return {
            "model_path": str(model_path),
            "exists": model_path.exists(),
            "files": file_status,
            "ready_variants": ready_variants,
            "missing_files": missing_files,
            "auto_download_used": auto_download_used,
            "model_source": model_source,
        }

    def preload(self, variant: Optional[str] = None) -> PipelineHandle:
        """æ˜¾å¼é¢„åŠ è½½æŒ‡å®šå˜ä½“."""

        normalized = self._normalize_variant(variant)
        return self._get_pipeline_handle(normalized)

    def get_handle(self, variant: str) -> PipelineHandle:
        """è·å–æˆ–åˆå§‹åŒ–æŒ‡å®šå˜ä½“çš„ pipelineï¼ˆæ”¯æŒæœªå½’ä¸€åŒ–çš„å˜ä½“åç§°ï¼‰ã€‚"""

        normalized = self._normalize_variant(variant)
        return self._get_pipeline_handle(normalized)

    def _get_pipeline_handle(self, variant: str) -> PipelineHandle:
        if variant not in self._pipelines:
            with self._lock:
                if variant not in self._pipelines:
                    self._pipelines[variant] = self._build_pipeline_handle(variant)
        return self._pipelines[variant]

    def _build_pipeline_handle(self, variant: str) -> PipelineHandle:
        """æ ¹æ®å˜ä½“åˆå§‹åŒ– pipeline å¹¶ç¼“å­˜.

        å½“å‰å®ç°ä»…æ”¯æŒ tiny_long å˜ä½“ã€‚
        """

        # å»¶è¿Ÿå¯¼å…¥ FlashVSR ç›¸å…³ä¾èµ–ï¼Œé¿å…åœ¨ä»…ä½¿ç”¨è¾…åŠ©æ–¹æ³•æˆ–ç³»ç»ŸçŠ¶æ€æŸ¥è¯¢æ—¶å°±è§¦å‘é‡å‹æ¨¡å‹åŠ è½½ã€‚
        from app.flashvsr_core import FlashVSRTinyLongPipeline, ModelManager
        from app.flashvsr_core.diffsynth.models.downloader import (
            download_customized_models,
        )
        from app.flashvsr_core.wan_utils import build_tcdecoder, Causal_LQ4x_Proj

        print(f"ğŸš€ åˆå§‹åŒ– FlashVSR {settings.FLASHVSR_VERSION} pipeline ({variant})...")
        model_path = settings.FLASHVSR_MODEL_PATH

        needed_files = list(self.BASE_MODEL_FILES)

        missing = [name for name in needed_files if not (model_path / name).exists()]
        prompt_missing = not self.PROMPT_TENSOR_FILE.exists()

        auto_download_used = False
        model_source = "local"

        if missing or prompt_missing:
            missing_desc = ", ".join(
                missing
                + ([FLASHVSR_TINY_LONG_PROMPT_FILE] if prompt_missing else [])
            )
            print(
                f"âš ï¸ æ£€æµ‹åˆ°ç¼ºå°‘ FlashVSR æƒé‡æ–‡ä»¶: {missing_desc} (æ ¹ç›®å½•: {model_path})ï¼Œ"
                f"å°è¯•ä» ModelScope ä»“åº“ `{FLASHVSR_TINY_LONG_REPO_ID}` è‡ªåŠ¨ä¸‹è½½..."
            )
            try:
                # ä»…ä¸‹è½½ç¼ºå¤±éƒ¨åˆ†ï¼Œé¿å…é‡å¤æ‹‰å–å·²å­˜åœ¨çš„æ–‡ä»¶ã€‚
                for filename in missing:
                    download_customized_models(
                        FLASHVSR_TINY_LONG_REPO_ID,
                        filename,
                        str(model_path),
                        downloading_priority=["ModelScope", "HuggingFace"],
                    )
                if prompt_missing:
                    download_customized_models(
                        FLASHVSR_TINY_LONG_REPO_ID,
                        FLASHVSR_TINY_LONG_PROMPT_FILE,
                        str(model_path),
                        downloading_priority=["ModelScope", "HuggingFace"],
                    )
            except Exception as exc:  # pragma: no cover - ç½‘ç»œ/ä¾èµ–é”™è¯¯è·¯å¾„
                raise FileNotFoundError(
                    "ç¼ºå°‘ FlashVSR æƒé‡æ–‡ä»¶ï¼Œä¸”ä» ModelScope è‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ‰‹åŠ¨æ”¾ç½®æƒé‡åˆ° "
                    f"{model_path}ã€‚åŸå§‹é”™è¯¯: {exc}"
                ) from exc

            # è‡ªåŠ¨ä¸‹è½½åé‡æ–°æ£€æŸ¥ä¸€æ¬¡ï¼Œç¡®ä¿æ‰€æœ‰å¿…éœ€æ–‡ä»¶å‡å·²å°±ç»ªã€‚
            missing = [name for name in needed_files if not (model_path / name).exists()]
            prompt_missing = not self.PROMPT_TENSOR_FILE.exists()
            if missing or prompt_missing:
                missing_desc = ", ".join(
                    missing
                    + ([FLASHVSR_TINY_LONG_PROMPT_FILE] if prompt_missing else [])
                )
                raise FileNotFoundError(
                    "ä» ModelScope è‡ªåŠ¨ä¸‹è½½åä»ç¼ºå°‘ FlashVSR æƒé‡æ–‡ä»¶: "
                    + missing_desc
                    + f" (æ ¹ç›®å½•: {model_path})ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æˆ–æ£€æŸ¥è·¯å¾„é…ç½®ã€‚"
                )

            auto_download_used = True
            model_source = "ModelScope"

        self._auto_download_used = auto_download_used
        self._auto_download_source = model_source

        # åˆ°è¿™é‡Œæƒé‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæœ¬åœ°åŠ è½½æ¨¡å‹å³å¯ã€‚
        mm = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        weights_to_load = [str(model_path / self.BASE_MODEL_FILES[0])]
        mm.load_models(weights_to_load)

        if not self.PROMPT_TENSOR_FILE.exists():
            raise FileNotFoundError(
                "ç¼ºå°‘ FlashVSR prompt tensor: "
                f"{self.PROMPT_TENSOR_FILE}. è¯·å°† posi_prompt.pth "
                "æ”¾ç½®åœ¨ models/FlashVSR-v1.1/ ä¸‹æˆ–é€šè¿‡ FLASHVSR_PROMPT_TENSOR_PATH è¦†ç›–ï¼Œè¯¦è§ docs/deployment.mdã€‚"
            )

        prompt_tensor = torch.load(self.PROMPT_TENSOR_FILE, map_location="cpu")

        pipeline_cls = FlashVSRTinyLongPipeline

        device = resolve_device()
        print(f"ğŸ“ ä½¿ç”¨è®¾å¤‡: {device}")
        if device.startswith("cuda"):
            gpu_index = 0
            try:
                if ":" in device:
                    gpu_index = int(device.split(":", 1)[1])
            except Exception:
                gpu_index = 0
            try:
                print(f"ğŸ® GPU: {torch.cuda.get_device_name(gpu_index)}")
            except Exception:
                pass

        pipe = pipeline_cls.from_model_manager(mm, device=device)

        cache_device, cache_reason = decide_cache_offload_device(device)
        pipe.set_cache_offload_device(cache_device)
        if cache_device:
            print(f"ğŸ’¾ KV cache offload â†’ {cache_device} ({cache_reason})")

        # é…ç½® LQ æŠ•å½±å±‚
        lq_proj = Causal_LQ4x_Proj(in_dim=3, out_dim=1536, layer_num=1).to(
            device, dtype=torch.bfloat16
        )
        lq_proj.load_state_dict(
            torch.load(model_path / "LQ_proj_in.ckpt", map_location="cpu"),
            strict=True,
        )
        lq_proj.to(device)
        pipe.denoising_model().LQ_proj_in = lq_proj

        # é…ç½® TCDecoder
        multi_scale_channels = [512, 256, 128, 128]
        pipe.TCDecoder = build_tcdecoder(
            new_channels=multi_scale_channels, new_latent_channels=16 + 768
        )
        pipe.TCDecoder.load_state_dict(
            torch.load(model_path / "TCDecoder.ckpt", map_location="cpu"),
            strict=False,
        )

        # å¯é€‰ï¼šæµæ°´çº¿å¹¶è¡Œï¼ˆå¤š GPUï¼‰
        pp_devices, pp_split = parse_pipeline_parallel()

        default_kwargs: dict[str, Any] = {}

        pipe.to(device)
        # å¯ç”¨æµæ°´çº¿å¹¶è¡Œæ—¶ï¼Œä¸å¯ç”¨ VRAM management é¿å…è®¾å¤‡é”™é…
        if pp_devices is None:
            pipe.enable_vram_management(num_persistent_param_in_dit=None)
        pipe.init_cross_kv(context_tensor=prompt_tensor)
        pipe.load_models_to_device(["dit", "vae"])

        # åˆå§‹åŒ–æµæ°´çº¿å¹¶è¡Œï¼ˆéœ€è¦åœ¨ init_cross_kv ä¹‹åï¼ŒæŠŠ cross-attn ç¼“å­˜ä¹Ÿè¿ç§»ï¼‰
        if pp_devices is not None and hasattr(pipe, "enable_pipeline_parallel"):
            try:
                pipe.enable_pipeline_parallel(pp_devices, split_index=pp_split)
                print(
                    "ğŸ”€ Pipeline parallel enabled on "
                    f"{pp_devices} (split @ block {pp_split if pp_split is not None else 'auto'})"
                )
                # ä¼ é€’é‡å è°ƒåº¦æ¨¡å¼ç»™ pipelineï¼ˆè‹¥å®ç°ï¼‰
                try:
                    overlap_mode = getattr(settings, "FLASHVSR_PP_OVERLAP_MODE", "basic")
                    if hasattr(pipe, "pp_overlap_mode"):
                        pipe.pp_overlap_mode = (overlap_mode or "basic").lower()
                        print(f"âš™ï¸ Pipeline overlap mode set to {pipe.pp_overlap_mode}")
                except Exception:
                    pass
            except Exception as e:
                print(f"âš ï¸ å¯ç”¨æµæ°´çº¿å¹¶è¡Œå¤±è´¥ï¼š{e}")
            # When PP is enabled, move TCDecoder to the last stage device to free GPU0 for Stage0
            try:
                dev1 = pp_devices[-1]
                if hasattr(pipe, "TCDecoder") and pipe.TCDecoder is not None:
                    pipe.TCDecoder.to(dev1)
                    print(f"ğŸ¯ TCDecoder moved to {dev1} for overlap")
            except Exception as e:
                print(f"âš ï¸ TCDecoder è¿ç§»åˆ° {pp_devices[-1]} å¤±è´¥ï¼š{e}")
        # Overlap scheduling for single video (optional)
        try:
            if getattr(settings, "FLASHVSR_PP_OVERLAP", False) and hasattr(
                pipe, "enable_pipeline_overlap"
            ):
                pipe.enable_pipeline_overlap(True)
                print("â© Pipeline overlap (Stage0/Stage1) enabled per window")
        except Exception as e:
            print(f"âš ï¸ å¯ç”¨æµæ°´çº¿é‡å å¤±è´¥ï¼š{e}")

        print(f"âœ… FlashVSR pipeline ({variant}) åˆå§‹åŒ–å®Œæˆ")
        return PipelineHandle(
            variant=variant,
            pipeline=pipe,
            device=device,
            default_kwargs=default_kwargs,
        )

    def _normalize_variant(self, variant: Optional[str]) -> str:
        value = (variant or settings.DEFAULT_MODEL_VARIANT).lower()
        if value not in self.SUPPORTED_VARIANTS:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ¨¡å‹å˜ä½“: {variant}. å¯é€‰: {', '.join(self.SUPPORTED_VARIANTS)}"
            )
        asset_status = self.inspect_assets().get("ready_variants", {})
        if not asset_status.get(value, False):
            raise RuntimeError(
                f"æ¨¡å‹å˜ä½“ {value} ç¼ºå°‘å¿…è¦æƒé‡ï¼Œè¯·å‚è€ƒ README ä¸‹è½½ FlashVSR {settings.FLASHVSR_VERSION} æƒé‡"
            )
        return value


_PIPELINE_MANAGER: Optional[FlashVSRPipelineManager] = None


def get_pipeline_manager() -> FlashVSRPipelineManager:
    global _PIPELINE_MANAGER
    if _PIPELINE_MANAGER is None:
        _PIPELINE_MANAGER = FlashVSRPipelineManager()
    return _PIPELINE_MANAGER


class FlashVSRJobRunner:
    """æ‰§è¡Œ FlashVSR æ¨ç†ä¸å¯¼å‡ºæµç¨‹."""

    def __init__(
        self,
        pipeline_manager: FlashVSRPipelineManager,
        exporter: VideoExporter,
    ) -> None:
        self._pipelines = pipeline_manager
        self._exporter = exporter

    @staticmethod
    def _should_use_chunk_writer(total_frames: int) -> bool:
        return total_frames > 0

    def run(
        self,
        input_path: str,
        output_path: str,
        params: FlashVSRJobParams,
        progress_callback: Optional[Callable[[int, int, float], None]] = None,
    ) -> dict:
        """å¤„ç†è§†é¢‘è¶…åˆ†è¾¨ç‡."""

        handle = self._pipelines.get_handle(params.model_variant)
        pipeline = handle.pipeline
        device = handle.device
        variant = handle.variant

        print(
            f"ğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘: {input_path} | æ¨¡å‹: FlashVSR {settings.FLASHVSR_VERSION} ({variant})"
        )
        start_time = time.time()

        # å‡†å¤‡è¾“å…¥
        if params.preserve_aspect_ratio:
            aspect_strategy: AspectStrategy = PadThenCropStrategy()
        else:
            aspect_strategy = CenterCrop128Strategy()

        def build_transform(w0: int, h0: int):
            return aspect_strategy.build_transform(w0, h0, params.scale)

        video_tensor, height, width, total_frames, fps = prepare_input(
            input_path,
            device,
            build_transform,
        )

        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {total_frames}å¸§, {fps}fps")

        progress: Optional[ProgressReporter] = None
        if progress_callback and total_frames:
            progress = ProgressReporter(total_frames=total_frames, on_update=progress_callback)
            # åˆå§‹ä¸€æ¬¡ 0 å¸§è¿›åº¦ï¼Œä¾¿äºä¸Šå±‚å±•ç¤ºé¢„ä¼°è€—æ—¶ã€‚
            progress.report(0)

        # å¤„ç†è§†é¢‘
        infer_start = time.time()
        pipeline_kwargs = {
            "prompt": "",
            "negative_prompt": "",
            "cfg_scale": 1.0,
            "num_inference_steps": 1,
            "seed": params.seed,
            "LQ_video": video_tensor,
            "num_frames": total_frames,
            "height": height,
            "width": width,
            "is_full_block": False,
            "if_buffer": True,
            "topk_ratio": params.sparse_ratio * 768 * 1280 / (height * width),
            "kv_ratio": 3.0,
            "local_range": params.local_range,
            "color_fix": True,
        }
        pipeline_kwargs.update(handle.default_kwargs)

        chunk_session = None
        supports_chunk_stream = (
            "frame_chunk_handler" in inspect.signature(pipeline.__call__).parameters
        )
        use_chunk_writer = self._should_use_chunk_writer(total_frames)
        effective_total_for_progress = (
            progress.total_frames if progress is not None else total_frames
        )

        if use_chunk_writer:
            chunk_session = self._exporter.create_chunk_session(
                output_path=output_path,
                fps=fps,
                total_frames=effective_total_for_progress,
                progress=progress,
                audio_path=params.audio_path,
                start_time=start_time,
            )
            # å¯¹äºæ”¯æŒåŸç”Ÿåˆ†ç‰‡æ¨ç†çš„ pipelineï¼Œé€šè¿‡ frame_chunk_handler å®æ—¶å†™å…¥ä¸å›è°ƒè¿›åº¦ï¼›
            # å¦åˆ™åœ¨æ¨ç†ç»“æŸåå†å°†å®Œæ•´è¾“å‡ºå¼ é‡æŒ‰åŒä¸€é€»è¾‘å†™å…¥åˆ†ç‰‡ï¼Œä¿æŒå¯¼å‡ºè·¯å¾„ä¸€è‡´ã€‚
            if supports_chunk_stream:
                pipeline_kwargs["frame_chunk_handler"] = chunk_session.handle_chunk

        cleanup_handle = video_tensor if hasattr(video_tensor, "cleanup") else None
        try:
            with torch.inference_mode():
                output_video = pipeline(**pipeline_kwargs)
        except Exception:
            # å‡ºé”™æ—¶ä¸å†è‡ªåŠ¨å¯¼å‡ºéƒ¨åˆ†ç»“æœï¼Œç”±ä¸Šå±‚æ ¹æ® chunks_* ç›®å½•æ˜¾å¼è§¦å‘å¯¼å‡ºã€‚
            if chunk_session is not None:
                chunk_session.abort()
            raise
        finally:
            if cleanup_handle is not None:
                cleanup_handle.cleanup()
        inference_time = time.time() - infer_start

        if chunk_session is not None:
            if not supports_chunk_stream:
                # pipeline ä¸æ”¯æŒ frame_chunk_handler æ—¶ï¼Œæ­¤å¤„æ‹¿åˆ°çš„æ˜¯å®Œæ•´è¾“å‡ºå¼ é‡ï¼Œ
                # é€šè¿‡ç»Ÿä¸€çš„ ChunkedExportSession å¯¼å‡ºï¼Œä»¥ä¿æŒä¸æµå¼åˆ†ç‰‡ç›¸åŒçš„å†™å…¥ä¸è¿›åº¦å›è°ƒé€»è¾‘ã€‚
                if output_video is not None:
                    # output_video å½¢çŠ¶ä¸º (C, T, H, W)ï¼Œåˆ†ç‰‡å¯¼å‡ºæœŸæœ› (B, C, T, H, W)
                    chunk_session.handle_chunk(output_video.unsqueeze(0))
            chunk_session.close()
            processed_frame_count = effective_total_for_progress
            if progress is not None:
                progress.done()
        else:
            processed_frame_count = self._exporter.export_full_tensor(
                frames=output_video,
                output_path=output_path,
                fps=fps,
                progress=progress,
                audio_path=params.audio_path,
                total_frames=effective_total_for_progress,
                start_time=start_time,
            )

        # å¯¼å‡ºé˜¶æ®µæŒ‰æºè§†é¢‘é•¿å®½æ¯”è¿›è¡Œå¯é€‰çš„è£å‰ªï¼ˆå»é™¤ padding çš„é»‘è¾¹ï¼Œä¸å†äºŒæ¬¡æ‹‰ä¼¸ç”»é¢ï¼‰ã€‚
        if params.preserve_aspect_ratio and params.source_width and params.source_height:
            strategy: AspectStrategy = PadThenCropStrategy()
            try:
                strategy.finalize_output(
                    path=output_path,
                    source_width=params.source_width,
                    source_height=params.source_height,
                    scale=params.scale,
                )
            except Exception as exc:
                print(f"âš ï¸ å¯¼å‡ºé˜¶æ®µæŒ‰åŸå§‹é•¿å®½æ¯”è£å‰ªå¤±è´¥: {exc}")

        total_time = time.time() - start_time

        print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {output_path}")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")

        if device == "cuda":
            torch.cuda.empty_cache()

        return {
            "width": width,
            "height": height,
            "total_frames": total_frames,
            "fps": fps,
            "processed_frames": processed_frame_count,
            "inference_time": inference_time,
            "processing_time": total_time,
            "model_variant": variant,
        }


class FlashVSRService:
    """FlashVSR æ¨ç†æœåŠ¡ï¼ˆå¯¹å¤– Facade)."""

    # å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰å¸¸é‡å¯¼å‡ºä½ç½®ã€‚
    SUPPORTED_VARIANTS: tuple[str, ...] = FlashVSRPipelineManager.SUPPORTED_VARIANTS
    BASE_MODEL_FILES: tuple[str, ...] = FlashVSRPipelineManager.BASE_MODEL_FILES
    FULL_ONLY_FILES: tuple[str, ...] = FlashVSRPipelineManager.FULL_ONLY_FILES
    PROMPT_TENSOR_FILE = FlashVSRPipelineManager.PROMPT_TENSOR_FILE

    _instance: Optional["FlashVSRService"] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self) -> None:
        if getattr(self, "_initialized", False):
            return
        self._pipelines = get_pipeline_manager()
        self._exporter = VideoExporter()
        self._runner = FlashVSRJobRunner(self._pipelines, self._exporter)
        self._initialized = True

    @classmethod
    def inspect_assets(cls) -> dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹æƒé‡æƒ…å†µï¼Œä¾›ç³»ç»ŸçŠ¶æ€å’Œè¯Šæ–­ä½¿ç”¨."""

        manager = get_pipeline_manager()
        return manager.inspect_assets()

    def process_video(
        self,
        input_path: str,
        output_path: str,
        scale: float = 4.0,
        sparse_ratio: float = 2.0,
        local_range: int = 11,
        seed: int = 0,
        model_variant: str = settings.DEFAULT_MODEL_VARIANT,
        progress_callback: Optional[Callable[[int, int, float], None]] = None,
        audio_path: Optional[str] = None,
        preserve_aspect_ratio: bool = False,
        source_width: Optional[int] = None,
        source_height: Optional[int] = None,
    ) -> dict:
        """å¤„ç†è§†é¢‘è¶…åˆ†è¾¨ç‡."""

        params = FlashVSRJobParams(
            scale=scale,
            sparse_ratio=sparse_ratio,
            local_range=local_range,
            seed=seed,
            model_variant=model_variant,
            audio_path=audio_path,
            preserve_aspect_ratio=preserve_aspect_ratio,
            source_width=source_width,
            source_height=source_height,
        )
        return self._runner.run(
            input_path=input_path,
            output_path=output_path,
            params=params,
            progress_callback=progress_callback,
        )

    def export_partial_from_chunks(self, expected_output_path: str) -> Optional[Path]:
        """
        åŸºäºç£ç›˜ä¸Šå·²æœ‰çš„åˆ†ç‰‡æ–‡ä»¶åˆå¹¶å¹¶å¯¼å‡ºä¸€ä¸ªéƒ¨åˆ†ç»“æœã€‚

        - ä¸»è¦ç”¨äºä»»åŠ¡å·²ç»ç»“æŸï¼ˆè¶…æ—¶ / å´©æºƒï¼‰åï¼Œæ ¹æ® chunks_* ç›®å½•ä¸­ç°æœ‰çš„åˆ†ç‰‡æ¢å¤è¿›åº¦ã€‚
        - ä¸ä¾èµ–ä»åœ¨å†…å­˜ä¸­çš„ ChunkedExportSessionã€‚
        """
        base_name = build_chunk_base_name(expected_output_path)
        root = settings.FLASHVSR_CHUNKED_SAVE_TMP_DIR
        if not root.exists():
            return None

        best_dir: Optional[Path] = None
        best_chunks: list[Path] = []

        for sub in root.iterdir():
            if not sub.is_dir() or not sub.name.startswith("chunks_"):
                continue
            candidates = sorted(sub.glob(f"{base_name}_chunk_*.mp4"))
            if candidates and len(candidates) > len(best_chunks):
                best_dir = sub
                best_chunks = candidates

        if not best_dir or not best_chunks:
            return None

        # ä¸ºé¿å…æœ€åä¸€ä¸ªæœªæ­£å¸¸å…³é—­çš„åˆ†ç‰‡å¯¼è‡´åˆå¹¶å¤±è´¥ï¼Œä¿å®ˆåœ°ä¸¢å¼ƒæœ€åä¸€ä¸ªã€‚
        usable = best_chunks[:-1] if len(best_chunks) > 1 else best_chunks
        if not usable:
            return None

        partial_path = Path(expected_output_path).with_name(
            f"{Path(expected_output_path).stem}_partial{Path(expected_output_path).suffix}"
        )
        # ä½¿ç”¨ä¸æ­£å¸¸æµç¨‹ç›¸åŒçš„åˆå¹¶é€»è¾‘ï¼Œå¹¶åœ¨å®Œæˆåæ¸…ç†è¿™äº›åˆ†ç‰‡ã€‚
        merge_video_chunks(usable, str(partial_path), audio_path=None)
        return partial_path

    def preload_variant(self, variant: Optional[str] = None) -> PipelineHandle:
        """æ˜¾å¼é¢„åŠ è½½æŒ‡å®šå˜ä½“."""

        return self._pipelines.preload(variant)
