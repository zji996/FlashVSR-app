"""FlashVSR æ¨ç†æœåŠ¡å°è£…."""

from __future__ import annotations

import inspect
import os
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from threading import Lock
from typing import Any, Callable, Optional

# é¿å… PyTorch é¢„ç•™çš„å¤§å—æ˜¾å­˜æ— æ³•å¤ç”¨ï¼Œé»˜è®¤å¯ç”¨å¯æ‰©å±•åˆ†æ®µåˆ†é…ã€‚
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

import torch

from app.config import settings
from app.flashvsr_core.diffsynth.configs.model_config import (
    FLASHVSR_TINY_LONG_BASE_FILES,
    FLASHVSR_TINY_LONG_EXTRA_FILES,
    FLASHVSR_TINY_LONG_PROMPT_FILE,
    FLASHVSR_TINY_LONG_REPO_ID,
)
from app.services.chunk_export import ChunkedExportSession, build_chunk_base_name
from app.services.flashvsr_device import (
    resolve_device,
    decide_cache_offload_device,
    parse_pipeline_parallel,
)
from app.services.flashvsr_io import (
    prepare_input,
    export_video_from_tensor,
    merge_video_chunks,
)

# Block-Sparse æ³¨æ„åŠ›ä¾èµ–çš„ CUDA æ‰©å±•è·¯å¾„ï¼ˆå®é™…å¯¼å…¥åœ¨ FlashVSR pipeline åˆå§‹åŒ–æ—¶è§¦å‘ï¼‰
BLOCK_SPARSE_PATH = settings.THIRD_PARTY_BLOCK_SPARSE_PATH
if str(BLOCK_SPARSE_PATH) not in sys.path:
    sys.path.insert(0, str(BLOCK_SPARSE_PATH))


@dataclass
class PipelineHandle:
    """ç¼“å­˜çš„ Pipeline å®ä¾‹ä¿¡æ¯."""

    variant: str
    pipeline: Any
    device: str
    default_kwargs: dict[str, Any]


class FlashVSRService:
    """FlashVSR æ¨ç†æœåŠ¡ï¼ˆå•ä¾‹ + å˜ä½“ç¼“å­˜)."""

    SUPPORTED_VARIANTS: tuple[str, ...] = ("tiny_long",)
    BASE_MODEL_FILES: tuple[str, ...] = FLASHVSR_TINY_LONG_BASE_FILES
    FULL_ONLY_FILES: tuple[str, ...] = FLASHVSR_TINY_LONG_EXTRA_FILES
    PROMPT_TENSOR_FILE = settings.FLASHVSR_PROMPT_TENSOR_PATH

    _instance: Optional["FlashVSRService"] = None
    _pipelines: dict[str, PipelineHandle] = {}
    _lock: Lock = Lock()
    _auto_download_used: bool = False
    _auto_download_source: Optional[str] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self) -> None:
        # Pipeline å»¶è¿ŸåŠ è½½ï¼Œé¦–æ¬¡è°ƒç”¨æŒ‡å®šå˜ä½“æ—¶å†åˆå§‹åŒ–
        pass

    @classmethod
    def inspect_assets(cls) -> dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹æƒé‡æƒ…å†µï¼Œä¾›ç³»ç»ŸçŠ¶æ€å’Œè¯Šæ–­ä½¿ç”¨."""

        model_path = settings.FLASHVSR_MODEL_PATH
        file_status: dict[str, bool] = {}

        for filename in cls.BASE_MODEL_FILES + cls.FULL_ONLY_FILES:
            file_status[filename] = (model_path / filename).exists()
        file_status[FLASHVSR_TINY_LONG_PROMPT_FILE] = cls.PROMPT_TENSOR_FILE.exists()

        def _ready(extra: tuple[str, ...] = ()) -> bool:
            base_ready = file_status[FLASHVSR_TINY_LONG_PROMPT_FILE] and all(
                file_status[name] for name in cls.BASE_MODEL_FILES
            )
            extra_ready = all(file_status[name] for name in extra)
            return base_ready and extra_ready

        ready_variants = {
            "tiny_long": _ready(),
        }
        missing_files = [name for name, ok in file_status.items() if not ok]

        if cls._auto_download_used:
            model_source = cls._auto_download_source or "ModelScope"
            auto_download_used = True
        else:
            auto_download_used = False
            if model_path.exists() and not missing_files:
                model_source = "local"
            else:
                model_source = None

        return {
            "model_path": str(model_path),
            "exists": model_path.exists(),
            "files": file_status,
            "ready_variants": ready_variants,
            "missing_files": missing_files,
            "auto_download_used": auto_download_used,
            "model_source": model_source,
        }

    def process_video(
        self,
        input_path: str,
        output_path: str,
        scale: float = 4.0,
        sparse_ratio: float = 2.0,
        local_range: int = 11,
        seed: int = 0,
        model_variant: str = settings.DEFAULT_MODEL_VARIANT,
        progress_callback: Optional[Callable[[int, int, float], None]] = None,
        audio_path: Optional[str] = None,
    ) -> dict:
        """å¤„ç†è§†é¢‘è¶…åˆ†è¾¨ç‡."""

        variant = self._normalize_variant(model_variant)
        handle = self._get_pipeline_handle(variant)
        pipeline = handle.pipeline
        device = handle.device

        print(
            f"ğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘: {input_path} | æ¨¡å‹: FlashVSR {settings.FLASHVSR_VERSION} ({variant})"
        )
        start_time = time.time()

        # å‡†å¤‡è¾“å…¥
        video_tensor, height, width, total_frames, fps = self._prepare_input(
            input_path, scale, device
        )

        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {total_frames}å¸§, {fps}fps")

        if progress_callback and total_frames:
            progress_callback(0, total_frames, 0.0)

        # å¤„ç†è§†é¢‘
        infer_start = time.time()
        pipeline_kwargs = {
            "prompt": "",
            "negative_prompt": "",
            "cfg_scale": 1.0,
            "num_inference_steps": 1,
            "seed": seed,
            "LQ_video": video_tensor,
            "num_frames": total_frames,
            "height": height,
            "width": width,
            "is_full_block": False,
            "if_buffer": True,
            "topk_ratio": sparse_ratio * 768 * 1280 / (height * width),
            "kv_ratio": 3.0,
            "local_range": local_range,
            "color_fix": True,
        }
        pipeline_kwargs.update(handle.default_kwargs)

        chunk_session: Optional[ChunkedExportSession] = None
        supports_chunk_stream = "frame_chunk_handler" in inspect.signature(pipeline.__call__).parameters
        if self._should_use_chunk_writer(total_frames) and supports_chunk_stream:
            chunk_session = ChunkedExportSession(
                output_path=output_path,
                fps=fps,
                total_frames=total_frames,
                start_time=start_time,
                progress_callback=progress_callback,
                audio_path=audio_path,
            )
            pipeline_kwargs["frame_chunk_handler"] = chunk_session.handle_chunk

        cleanup_handle = video_tensor if hasattr(video_tensor, "cleanup") else None
        try:
            with torch.inference_mode():
                output_video = pipeline(**pipeline_kwargs)
        except Exception as exc:
            # å‡ºé”™æ—¶ä¸å†è‡ªåŠ¨å¯¼å‡ºéƒ¨åˆ†ç»“æœï¼Œç”±ä¸Šå±‚æ ¹æ® chunks_* ç›®å½•æ˜¾å¼è§¦å‘å¯¼å‡ºã€‚
            if chunk_session:
                chunk_session.abort()
            raise
        finally:
            if cleanup_handle is not None:
                cleanup_handle.cleanup()
        inference_time = time.time() - infer_start

        if chunk_session:
            chunk_session.close()
            processed_frame_count = total_frames
        else:
            processed_frame_count = export_video_from_tensor(
                output_video=output_video,
                output_path=output_path,
                fps=fps,
                total_frames=total_frames,
                start_time=start_time,
                progress_callback=progress_callback,
                audio_path=audio_path,
            )

        total_time = time.time() - start_time

        print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {output_path}")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")

        if device == "cuda":
            torch.cuda.empty_cache()

        return {
            "width": width,
            "height": height,
            "total_frames": total_frames,
            "fps": fps,
            "processed_frames": processed_frame_count,
            "inference_time": inference_time,
            "processing_time": total_time,
            "model_variant": variant,
        }

    def _prepare_input(self, path: str, scale: float, device: str):
        """å‡†å¤‡è¾“å…¥è§†é¢‘ tensor."""
        return prepare_input(path, scale, device)

    @staticmethod
    def _should_use_chunk_writer(total_frames: int) -> bool:
        min_frames = settings.FLASHVSR_CHUNKED_SAVE_MIN_FRAMES
        return min_frames > 0 and total_frames >= min_frames

    def _merge_video_chunks(self, chunk_paths: list[Path], output_path: str, audio_path: Optional[str] = None) -> None:
        if not chunk_paths:
            raise RuntimeError("æœªç”Ÿæˆå¯ç”¨äºåˆå¹¶çš„åˆ†ç‰‡")
        chunk_paths.sort(key=lambda path: path.name)
        chunk_dir = chunk_paths[0].parent
        if len(chunk_paths) == 1:
            merged_video = chunk_paths[0]
        else:
            list_file = chunk_dir / f"{Path(output_path).stem}_chunks.txt"
            with open(list_file, "w", encoding="utf-8") as handle:
                for path in chunk_paths:
                    handle.write(f"file '{path}'\n")

            tmp_merged = chunk_dir / f"{Path(output_path).stem}_video_only.mp4"
            cmd = [
                settings.FFMPEG_BINARY,
                "-y",
                "-f",
                "concat",
                "-safe",
                "0",
                "-i",
                str(list_file),
                "-c",
                "copy",
                str(tmp_merged),
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                raise RuntimeError(
                    f"FFmpeg åˆå¹¶åˆ†ç‰‡å¤±è´¥ï¼ˆ{result.returncode}ï¼‰: "
                    f"{result.stderr.strip() or result.stdout.strip()}"
                )

            list_file.unlink(missing_ok=True)
            for path in chunk_paths:
                path.unlink(missing_ok=True)
            merged_video = tmp_merged

        if audio_path and Path(audio_path).exists():
            self._mux_audio(str(merged_video), audio_path, output_path)
            if merged_video != Path(output_path):
                Path(merged_video).unlink(missing_ok=True)
        else:
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            shutil.move(str(merged_video), output_path)
        try:
            chunk_dir.rmdir()
        except OSError:
            pass

    @staticmethod
    def _cleanup_chunk_artifacts(paths: list[Path]) -> None:
        """å°½åŠ›åˆ é™¤å¼‚å¸¸æƒ…å†µä¸‹é—ç•™çš„åˆ†ç‰‡æ–‡ä»¶."""
        for path in paths:
            path.unlink(missing_ok=True)
        if paths:
            try:
                paths[0].parent.rmdir()
            except OSError:
                pass

    def export_partial_from_chunks(self, expected_output_path: str) -> Optional[Path]:
        """
        åŸºäºç£ç›˜ä¸Šå·²æœ‰çš„åˆ†ç‰‡æ–‡ä»¶åˆå¹¶å¹¶å¯¼å‡ºä¸€ä¸ªéƒ¨åˆ†ç»“æœã€‚

        - ä¸»è¦ç”¨äºä»»åŠ¡å·²ç»ç»“æŸï¼ˆè¶…æ—¶ / å´©æºƒï¼‰åï¼Œæ ¹æ® chunks_* ç›®å½•ä¸­ç°æœ‰çš„åˆ†ç‰‡æ¢å¤è¿›åº¦ã€‚
        - ä¸ä¾èµ–ä»åœ¨å†…å­˜ä¸­çš„ ChunkedExportSessionã€‚
        """
        base_name = build_chunk_base_name(expected_output_path)
        root = settings.FLASHVSR_CHUNKED_SAVE_TMP_DIR
        if not root.exists():
            return None

        best_dir: Optional[Path] = None
        best_chunks: list[Path] = []

        for sub in root.iterdir():
            if not sub.is_dir() or not sub.name.startswith("chunks_"):
                continue
            candidates = sorted(sub.glob(f"{base_name}_chunk_*.mp4"))
            if candidates and len(candidates) > len(best_chunks):
                best_dir = sub
                best_chunks = candidates

        if not best_dir or not best_chunks:
            return None

        # ä¸ºé¿å…æœ€åä¸€ä¸ªæœªæ­£å¸¸å…³é—­çš„åˆ†ç‰‡å¯¼è‡´åˆå¹¶å¤±è´¥ï¼Œä¿å®ˆåœ°ä¸¢å¼ƒæœ€åä¸€ä¸ªã€‚
        usable = best_chunks[:-1] if len(best_chunks) > 1 else best_chunks
        if not usable:
            return None

        partial_path = Path(expected_output_path).with_name(
            f"{Path(expected_output_path).stem}_partial{Path(expected_output_path).suffix}"
        )
        # ä½¿ç”¨ä¸æ­£å¸¸æµç¨‹ç›¸åŒçš„åˆå¹¶é€»è¾‘ï¼Œå¹¶åœ¨å®Œæˆåæ¸…ç†è¿™äº›åˆ†ç‰‡ã€‚
        merge_video_chunks(usable, str(partial_path), audio_path=None)
        return partial_path

    def preload_variant(self, variant: Optional[str] = None) -> PipelineHandle:
        """æ˜¾å¼é¢„åŠ è½½æŒ‡å®šå˜ä½“."""

        normalized = self._normalize_variant(variant)
        return self._get_pipeline_handle(normalized)

    def _get_pipeline_handle(self, variant: str) -> PipelineHandle:
        """è·å–æˆ–åˆå§‹åŒ–æŒ‡å®šå˜ä½“çš„ pipeline."""

        if variant not in self._pipelines:
            with self._lock:
                if variant not in self._pipelines:
                    self._pipelines[variant] = self._build_pipeline_handle(variant)
        return self._pipelines[variant]

    def _build_pipeline_handle(self, variant: str) -> PipelineHandle:
        """æ ¹æ®å˜ä½“åˆå§‹åŒ– pipeline å¹¶ç¼“å­˜.

        å½“å‰å®ç°ä»…æ”¯æŒ tiny_long å˜ä½“ã€‚
        """

        # å»¶è¿Ÿå¯¼å…¥ FlashVSR ç›¸å…³ä¾èµ–ï¼Œé¿å…åœ¨ä»…ä½¿ç”¨è¾…åŠ©æ–¹æ³•æˆ–ç³»ç»ŸçŠ¶æ€æŸ¥è¯¢æ—¶å°±è§¦å‘é‡å‹æ¨¡å‹åŠ è½½ã€‚
        from app.flashvsr_core import FlashVSRTinyLongPipeline, ModelManager
        from app.flashvsr_core.diffsynth.models.downloader import (
            download_customized_models,
        )
        from app.flashvsr_core.wan_utils import build_tcdecoder, Causal_LQ4x_Proj

        print(f"ğŸš€ åˆå§‹åŒ– FlashVSR {settings.FLASHVSR_VERSION} pipeline ({variant})...")
        model_path = settings.FLASHVSR_MODEL_PATH

        needed_files = list(self.BASE_MODEL_FILES)

        missing = [name for name in needed_files if not (model_path / name).exists()]
        prompt_missing = not self.PROMPT_TENSOR_FILE.exists()

        auto_download_used = False
        model_source = "local"

        if missing or prompt_missing:
            missing_desc = ", ".join(
                missing
                + ([FLASHVSR_TINY_LONG_PROMPT_FILE] if prompt_missing else [])
            )
            print(
                f"âš ï¸ æ£€æµ‹åˆ°ç¼ºå°‘ FlashVSR æƒé‡æ–‡ä»¶: {missing_desc} (æ ¹ç›®å½•: {model_path})ï¼Œ"
                f"å°è¯•ä» ModelScope ä»“åº“ `{FLASHVSR_TINY_LONG_REPO_ID}` è‡ªåŠ¨ä¸‹è½½..."
            )
            try:
                # ä»…ä¸‹è½½ç¼ºå¤±éƒ¨åˆ†ï¼Œé¿å…é‡å¤æ‹‰å–å·²å­˜åœ¨çš„æ–‡ä»¶ã€‚
                for filename in missing:
                    download_customized_models(
                        FLASHVSR_TINY_LONG_REPO_ID,
                        filename,
                        str(model_path),
                        downloading_priority=["ModelScope", "HuggingFace"],
                    )
                if prompt_missing:
                    download_customized_models(
                        FLASHVSR_TINY_LONG_REPO_ID,
                        FLASHVSR_TINY_LONG_PROMPT_FILE,
                        str(model_path),
                        downloading_priority=["ModelScope", "HuggingFace"],
                    )
            except Exception as exc:  # pragma: no cover - ç½‘ç»œ/ä¾èµ–é”™è¯¯è·¯å¾„
                raise FileNotFoundError(
                    "ç¼ºå°‘ FlashVSR æƒé‡æ–‡ä»¶ï¼Œä¸”ä» ModelScope è‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ‰‹åŠ¨æ”¾ç½®æƒé‡åˆ° "
                    f"{model_path}ã€‚åŸå§‹é”™è¯¯: {exc}"
                ) from exc

            # è‡ªåŠ¨ä¸‹è½½åé‡æ–°æ£€æŸ¥ä¸€æ¬¡ï¼Œç¡®ä¿æ‰€æœ‰å¿…éœ€æ–‡ä»¶å‡å·²å°±ç»ªã€‚
            missing = [name for name in needed_files if not (model_path / name).exists()]
            prompt_missing = not self.PROMPT_TENSOR_FILE.exists()
            if missing or prompt_missing:
                missing_desc = ", ".join(
                    missing
                    + ([FLASHVSR_TINY_LONG_PROMPT_FILE] if prompt_missing else [])
                )
                raise FileNotFoundError(
                    "ä» ModelScope è‡ªåŠ¨ä¸‹è½½åä»ç¼ºå°‘ FlashVSR æƒé‡æ–‡ä»¶: "
                    + missing_desc
                    + f" (æ ¹ç›®å½•: {model_path})ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æˆ–æ£€æŸ¥è·¯å¾„é…ç½®ã€‚"
                )

            auto_download_used = True
            model_source = "ModelScope"

        type(self)._auto_download_used = auto_download_used
        type(self)._auto_download_source = model_source

        # åˆ°è¿™é‡Œæƒé‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæœ¬åœ°åŠ è½½æ¨¡å‹å³å¯ã€‚
        mm = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        weights_to_load = [str(model_path / self.BASE_MODEL_FILES[0])]
        mm.load_models(weights_to_load)

        if not self.PROMPT_TENSOR_FILE.exists():
            raise FileNotFoundError(
                "ç¼ºå°‘ FlashVSR prompt tensor: "
                f"{self.PROMPT_TENSOR_FILE}. è¯·å°† posi_prompt.pth "
                "æ”¾ç½®åœ¨ models/FlashVSR-v1.1/ ä¸‹æˆ–é€šè¿‡ FLASHVSR_PROMPT_TENSOR_PATH è¦†ç›–ï¼Œè¯¦è§ docs/deployment.mdã€‚"
            )

        prompt_tensor = torch.load(self.PROMPT_TENSOR_FILE, map_location="cpu")

        pipeline_cls = FlashVSRTinyLongPipeline

        device = resolve_device()
        print(f"ğŸ“ ä½¿ç”¨è®¾å¤‡: {device}")
        if device.startswith("cuda"):
            gpu_index = 0
            try:
                if ":" in device:
                    gpu_index = int(device.split(":", 1)[1])
            except Exception:
                gpu_index = 0
            try:
                print(f"ğŸ® GPU: {torch.cuda.get_device_name(gpu_index)}")
            except Exception:
                pass

        pipe = pipeline_cls.from_model_manager(mm, device=device)

        cache_device, cache_reason = decide_cache_offload_device(device)
        pipe.set_cache_offload_device(cache_device)
        if cache_device:
            print(f"ğŸ’¾ KV cache offload â†’ {cache_device} ({cache_reason})")

        # é…ç½® LQ æŠ•å½±å±‚
        lq_proj = Causal_LQ4x_Proj(in_dim=3, out_dim=1536, layer_num=1).to(
            device, dtype=torch.bfloat16
        )
        lq_proj.load_state_dict(
            torch.load(model_path / "LQ_proj_in.ckpt", map_location="cpu"),
            strict=True,
        )
        lq_proj.to(device)
        pipe.denoising_model().LQ_proj_in = lq_proj

        # é…ç½® TCDecoder
        multi_scale_channels = [512, 256, 128, 128]
        pipe.TCDecoder = build_tcdecoder(
            new_channels=multi_scale_channels, new_latent_channels=16 + 768
        )
        pipe.TCDecoder.load_state_dict(
            torch.load(model_path / "TCDecoder.ckpt", map_location="cpu"),
            strict=False,
        )

        # å¯é€‰ï¼šæµæ°´çº¿å¹¶è¡Œï¼ˆå¤š GPUï¼‰
        pp_devices, pp_split = parse_pipeline_parallel()

        default_kwargs: dict[str, Any] = {}

        pipe.to(device)
        # å¯ç”¨æµæ°´çº¿å¹¶è¡Œæ—¶ï¼Œä¸å¯ç”¨ VRAM management é¿å…è®¾å¤‡é”™é…
        if pp_devices is None:
            pipe.enable_vram_management(num_persistent_param_in_dit=None)
        pipe.init_cross_kv(context_tensor=prompt_tensor)
        pipe.load_models_to_device(["dit", "vae"])

        # åˆå§‹åŒ–æµæ°´çº¿å¹¶è¡Œï¼ˆéœ€è¦åœ¨ init_cross_kv ä¹‹åï¼ŒæŠŠ cross-attn ç¼“å­˜ä¹Ÿè¿ç§»ï¼‰
        if pp_devices is not None and hasattr(pipe, "enable_pipeline_parallel"):
            try:
                pipe.enable_pipeline_parallel(pp_devices, split_index=pp_split)
                print(f"ğŸ”€ Pipeline parallel enabled on {pp_devices} (split @ block {pp_split if pp_split is not None else 'auto'})")
                # ä¼ é€’é‡å è°ƒåº¦æ¨¡å¼ç»™ pipelineï¼ˆè‹¥å®ç°ï¼‰
                try:
                    overlap_mode = getattr(settings, "FLASHVSR_PP_OVERLAP_MODE", "basic")
                    if hasattr(pipe, "pp_overlap_mode"):
                        pipe.pp_overlap_mode = (overlap_mode or "basic").lower()
                        print(f"âš™ï¸ Pipeline overlap mode set to {pipe.pp_overlap_mode}")
                except Exception:
                    pass
            except Exception as e:
                print(f"âš ï¸ å¯ç”¨æµæ°´çº¿å¹¶è¡Œå¤±è´¥ï¼š{e}")
            # When PP is enabled, move TCDecoder to the last stage device to free GPU0 for Stage0
            try:
                dev1 = pp_devices[-1]
                if hasattr(pipe, "TCDecoder") and pipe.TCDecoder is not None:
                    pipe.TCDecoder.to(dev1)
                    print(f"ğŸ¯ TCDecoder moved to {dev1} for overlap")
            except Exception as e:
                print(f"âš ï¸ TCDecoder è¿ç§»åˆ° {pp_devices[-1]} å¤±è´¥ï¼š{e}")
        # Overlap scheduling for single video (optional)
        try:
            if getattr(settings, "FLASHVSR_PP_OVERLAP", False) and hasattr(pipe, "enable_pipeline_overlap"):
                pipe.enable_pipeline_overlap(True)
                print("â© Pipeline overlap (Stage0/Stage1) enabled per window")
        except Exception as e:
            print(f"âš ï¸ å¯ç”¨æµæ°´çº¿é‡å å¤±è´¥ï¼š{e}")

        print(f"âœ… FlashVSR pipeline ({variant}) åˆå§‹åŒ–å®Œæˆ")
        return PipelineHandle(
            variant=variant,
            pipeline=pipe,
            device=device,
            default_kwargs=default_kwargs,
        )

    def _normalize_variant(self, variant: Optional[str]) -> str:
        value = (variant or settings.DEFAULT_MODEL_VARIANT).lower()
        if value not in self.SUPPORTED_VARIANTS:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ¨¡å‹å˜ä½“: {variant}. å¯é€‰: {', '.join(self.SUPPORTED_VARIANTS)}"
            )
        asset_status = self.inspect_assets().get("ready_variants", {})
        if not asset_status.get(value, False):
            raise RuntimeError(
                f"æ¨¡å‹å˜ä½“ {value} ç¼ºå°‘å¿…è¦æƒé‡ï¼Œè¯·å‚è€ƒ README ä¸‹è½½ FlashVSR {settings.FLASHVSR_VERSION} æƒé‡"
            )
        return value
