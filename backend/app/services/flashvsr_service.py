"""FlashVSR æ¨ç†æœåŠ¡å°è£…."""

from __future__ import annotations

import inspect
import os
import sys
import time
import shutil
import subprocess
from dataclasses import dataclass
from pathlib import Path
from threading import Lock
from typing import Any, Callable, Optional

# é¿å… PyTorch é¢„ç•™çš„å¤§å—æ˜¾å­˜æ— æ³•å¤ç”¨ï¼Œé»˜è®¤å¯ç”¨å¯æ‰©å±•åˆ†æ®µåˆ†é…ã€‚
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

import imageio
import numpy as np
import torch
from einops import rearrange
from PIL import Image
from tqdm import tqdm

from app.config import settings
from app.services.chunk_export import ChunkedExportSession
from app.services.video_streaming import StreamingVideoTensor
from app.flashvsr_core import FlashVSRTinyLongPipeline, ModelManager
from app.flashvsr_core.wan_utils import build_tcdecoder, Causal_LQ4x_Proj

# Block-Sparse æ³¨æ„åŠ›ä¾èµ–çš„ CUDA æ‰©å±•
BLOCK_SPARSE_PATH = settings.THIRD_PARTY_BLOCK_SPARSE_PATH
if str(BLOCK_SPARSE_PATH) not in sys.path:
    sys.path.insert(0, str(BLOCK_SPARSE_PATH))


@dataclass
class PipelineHandle:
    """ç¼“å­˜çš„ Pipeline å®ä¾‹ä¿¡æ¯."""

    variant: str
    pipeline: Any
    device: str
    default_kwargs: dict[str, Any]


class FlashVSRService:
    """FlashVSR æ¨ç†æœåŠ¡ï¼ˆå•ä¾‹ + å˜ä½“ç¼“å­˜)."""

    SUPPORTED_VARIANTS: tuple[str, ...] = ("tiny_long",)
    BASE_MODEL_FILES: tuple[str, ...] = (
        "diffusion_pytorch_model_streaming_dmd.safetensors",
        "LQ_proj_in.ckpt",
        "TCDecoder.ckpt",
    )
    FULL_ONLY_FILES: tuple[str, ...] = ("Wan2.1_VAE.pth",)
    PROMPT_TENSOR_FILE = settings.FLASHVSR_PROMPT_TENSOR_PATH

    _instance: Optional["FlashVSRService"] = None
    _pipelines: dict[str, PipelineHandle] = {}
    _lock: Lock = Lock()

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self) -> None:
        # Pipeline å»¶è¿ŸåŠ è½½ï¼Œé¦–æ¬¡è°ƒç”¨æŒ‡å®šå˜ä½“æ—¶å†åˆå§‹åŒ–
        pass

    @classmethod
    def inspect_assets(cls) -> dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹æƒé‡æƒ…å†µï¼Œä¾›ç³»ç»ŸçŠ¶æ€å’Œè¯Šæ–­ä½¿ç”¨."""

        model_path = settings.FLASHVSR_MODEL_PATH
        file_status: dict[str, bool] = {}

        for filename in cls.BASE_MODEL_FILES + cls.FULL_ONLY_FILES:
            file_status[filename] = (model_path / filename).exists()
        file_status["posi_prompt.pth"] = cls.PROMPT_TENSOR_FILE.exists()

        def _ready(extra: tuple[str, ...] = ()) -> bool:
            base_ready = file_status["posi_prompt.pth"] and all(
                file_status[name] for name in cls.BASE_MODEL_FILES
            )
            extra_ready = all(file_status[name] for name in extra)
            return base_ready and extra_ready

        ready_variants = {
            "tiny_long": _ready(),
        }
        missing_files = [name for name, ok in file_status.items() if not ok]

        return {
            "model_path": str(model_path),
            "exists": model_path.exists(),
            "files": file_status,
            "ready_variants": ready_variants,
            "missing_files": missing_files,
        }

    def process_video(
        self,
        input_path: str,
        output_path: str,
        scale: float = 4.0,
        sparse_ratio: float = 2.0,
        local_range: int = 11,
        seed: int = 0,
        model_variant: str = settings.DEFAULT_MODEL_VARIANT,
        progress_callback: Optional[Callable[[int, int, float], None]] = None,
        audio_path: Optional[str] = None,
    ) -> dict:
        """å¤„ç†è§†é¢‘è¶…åˆ†è¾¨ç‡."""

        variant = self._normalize_variant(model_variant)
        handle = self._get_pipeline_handle(variant)
        pipeline = handle.pipeline
        device = handle.device

        print(
            f"ğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘: {input_path} | æ¨¡å‹: FlashVSR {settings.FLASHVSR_VERSION} ({variant})"
        )
        start_time = time.time()

        # å‡†å¤‡è¾“å…¥
        video_tensor, height, width, total_frames, fps = self._prepare_input(
            input_path, scale, device
        )

        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {total_frames}å¸§, {fps}fps")

        if progress_callback and total_frames:
            progress_callback(0, total_frames, 0.0)

        # å¤„ç†è§†é¢‘
        infer_start = time.time()
        pipeline_kwargs = {
            "prompt": "",
            "negative_prompt": "",
            "cfg_scale": 1.0,
            "num_inference_steps": 1,
            "seed": seed,
            "LQ_video": video_tensor,
            "num_frames": total_frames,
            "height": height,
            "width": width,
            "is_full_block": False,
            "if_buffer": True,
            "topk_ratio": sparse_ratio * 768 * 1280 / (height * width),
            "kv_ratio": 3.0,
            "local_range": local_range,
            "color_fix": True,
        }
        pipeline_kwargs.update(handle.default_kwargs)

        chunk_session: Optional[ChunkedExportSession] = None
        supports_chunk_stream = "frame_chunk_handler" in inspect.signature(pipeline.__call__).parameters
        if self._should_use_chunk_writer(total_frames) and supports_chunk_stream:
            chunk_session = ChunkedExportSession(
                service=self,
                output_path=output_path,
                fps=fps,
                total_frames=total_frames,
                start_time=start_time,
                progress_callback=progress_callback,
                audio_path=audio_path,
            )
            pipeline_kwargs["frame_chunk_handler"] = chunk_session.handle_chunk

        cleanup_handle = video_tensor if hasattr(video_tensor, "cleanup") else None
        try:
            with torch.inference_mode():
                output_video = pipeline(**pipeline_kwargs)
        except Exception:
            if chunk_session:
                chunk_session.abort()
            raise
        finally:
            if cleanup_handle is not None:
                cleanup_handle.cleanup()
        inference_time = time.time() - infer_start

        if chunk_session:
            chunk_session.close()
            processed_frame_count = total_frames
        else:
            processed_frame_count = self._export_video(
                output_video=output_video,
                output_path=output_path,
                fps=fps,
                total_frames=total_frames,
                start_time=start_time,
                progress_callback=progress_callback,
                audio_path=audio_path,
            )

        total_time = time.time() - start_time

        print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {output_path}")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")

        if device == "cuda":
            torch.cuda.empty_cache()

        return {
            "width": width,
            "height": height,
            "total_frames": total_frames,
            "fps": fps,
            "processed_frames": processed_frame_count,
            "inference_time": inference_time,
            "processing_time": total_time,
            "model_variant": variant,
        }

    def _prepare_input(self, path: str, scale: float, device: str):
        """å‡†å¤‡è¾“å…¥è§†é¢‘ tensor."""
        dtype = torch.bfloat16

        # è¯»å–è§†é¢‘
        reader = imageio.get_reader(path)
        first_frame = Image.fromarray(reader.get_data(0)).convert('RGB')
        w0, h0 = first_frame.size

        # è·å–å…ƒæ•°æ®
        meta = {}
        try:
            meta = reader.get_meta_data()
        except Exception:
            pass

        fps_val = meta.get('fps', 30)
        fps = int(round(fps_val)) if isinstance(fps_val, (int, float)) else 30

        # è·å–æ€»å¸§æ•°
        total_frames = self._count_frames(reader, meta)

        print(f"åŸå§‹åˆ†è¾¨ç‡: {w0}x{h0}, åŸå§‹å¸§æ•°: {total_frames}, FPS: {fps}")

        # è®¡ç®—ç›®æ ‡å°ºå¯¸
        sW, sH, tW, tH = self._compute_scaled_dims(w0, h0, scale)
        print(f"ç›®æ ‡åˆ†è¾¨ç‡: {tW}x{tH} (ç¼©æ”¾ {scale}x)")

        # è¯»å–æ‰€æœ‰å¸§
        frames = []
        # å½“æ¨ç†åœ¨ GPU ä¸Šè¿è¡Œæ—¶ï¼Œä¸è¦åœ¨è¿™é‡ŒæŠŠæ‰€æœ‰å¸§ä¸€æ¬¡æ€§æ¬åˆ°æ˜¾å­˜ã€‚
        # å…ˆæ¨è¿› CPUï¼Œåç»­æŒ‰å— `.to(self.device)`ï¼Œå¯ä»¥æŠŠ ~4GB çš„ 3K@105 å¸§å¸¸é©»æ˜¾å­˜æ¶ˆé™¤æ‰ã€‚
        target_frame_device = "cpu" if device == "cuda" else device
        indices = list(range(total_frames)) + [total_frames - 1] * 4
        F = self._largest_8n1_leq(len(indices))
        indices = indices[:F]

        print(f"å¤„ç†å¸§æ•°: {F}")

        use_streaming = self._should_stream_video(F, tH, tW, dtype)

        reader_owned = True
        try:
            if use_streaming:
                video_tensor = self._build_streaming_video_tensor(
                    reader,
                    indices,
                    scale,
                    tW,
                    tH,
                    dtype,
                    target_frame_device,
                )
                reader_owned = False
            else:
                for i in tqdm(indices, desc="åŠ è½½è§†é¢‘å¸§"):
                    frames.append(
                        self._load_frame_tensor(
                            reader,
                            i,
                            scale,
                            tW,
                            tH,
                            dtype,
                            target_frame_device,
                        )
                    )
                video_tensor = torch.stack(frames, 0).permute(1, 0, 2, 3).unsqueeze(0)
                if device == "cuda":
                    video_tensor = video_tensor.pin_memory()
        finally:
            if reader_owned:
                reader.close()

        return video_tensor, tH, tW, F, fps

    @staticmethod
    def _count_frames(reader, meta):
        """è®¡ç®—è§†é¢‘æ€»å¸§æ•°."""
        try:
            nf = meta.get('nframes', None)
            if isinstance(nf, int) and nf > 0:
                return nf
        except Exception:
            pass

        try:
            return reader.count_frames()
        except Exception:
            n = 0
            try:
                while True:
                    reader.get_data(n)
                    n += 1
            except Exception:
                return n

    @staticmethod
    def _compute_scaled_dims(w0: int, h0: int, scale: float, multiple: int = 128):
        """
        è®¡ç®—ç¼©æ”¾åçš„å°ºå¯¸ã€‚

        - å…ˆæŒ‰ scale è®¡ç®—æ”¾å¤§åçš„å°ºå¯¸ (sW, sH)ã€‚
        - å†å‘ä¸‹å¯¹é½åˆ° multiple çš„å€æ•°ï¼Œç”¨äºæ»¡è¶³ FlashVSR çš„å—å¤§å°çº¦æŸã€‚
        - è¿™é‡Œä¿æŒä¸å®˜æ–¹ FlashVSR WanVideo æ¨¡å‹ä¸€è‡´ï¼Œä½¿ç”¨ multiple=128ï¼›
          è¿™æ ·åœ¨ VAE ä¸‹é‡‡æ · (Ã—1/8) å’Œ 3D patch (1,2,2) ä¹‹åï¼Œç‰¹å¾å›¾å°ºå¯¸ä¾ç„¶èƒ½è¢«
          self-attention çš„çª—å£ (2,8,8) æ•´é™¤ï¼Œé¿å… â€œDims must divide by window sizeâ€ é”™è¯¯ã€‚
        """
        sW = int(round(w0 * scale))
        sH = int(round(h0 * scale))

        tW = (sW // multiple) * multiple
        tH = (sH // multiple) * multiple

        return sW, sH, tW, tH

    @staticmethod
    def _upscale_and_crop(img: Image.Image, scale: float, tW: int, tH: int):
        """æ”¾å¤§å¹¶å±…ä¸­è£å‰ª."""
        w0, h0 = img.size
        sW = int(round(w0 * scale))
        sH = int(round(h0 * scale))

        up = img.resize((sW, sH), Image.BICUBIC)
        l = (sW - tW) // 2
        t = (sH - tH) // 2
        return up.crop((l, t, l + tW, t + tH))

    @staticmethod
    def _pil_to_tensor(img: Image.Image, dtype, device):
        """PIL å›¾åƒè½¬ tensor."""
        # ä½¿ç”¨æ˜¾å¼æ‹·è´ä¿è¯ NumPy æ•°ç»„æ˜¯å¯å†™çš„ï¼Œé¿å… PyTorch å…³äº
        # "non-writable tensor" çš„è­¦å‘Šï¼ŒåŒæ—¶ä¿æŒ dtype/layout ä¸å˜ã€‚
        arr = np.array(img, dtype=np.uint8, copy=True)
        t = torch.from_numpy(arr).to(
            device=device, dtype=torch.float32
        )
        t = t.permute(2, 0, 1) / 255.0 * 2.0 - 1.0
        return t.to(dtype)

    @staticmethod
    def _tensor2video(frames):
        """Tensor è½¬è§†é¢‘å¸§."""
        frames = rearrange(frames, "C T H W -> T H W C")
        frames = ((frames.float() + 1) * 127.5).clip(0, 255).cpu().numpy().astype(np.uint8)
        return [Image.fromarray(frame) for frame in frames]

    @staticmethod
    def _save_video(
        frames,
        save_path: str,
        fps: int = 30,
        quality: int = 6,
        progress_callback: Optional[Callable[[int, int, float], None]] = None,
        total_frames: Optional[int] = None,
        start_time: Optional[float] = None,
    ):
        """ä¿å­˜è§†é¢‘."""
        target_total = total_frames or len(frames)
        begin = start_time or time.time()

        save_dir = Path(save_path).parent
        save_dir.mkdir(parents=True, exist_ok=True)

        writer = imageio.get_writer(save_path, fps=fps, quality=quality)
        try:
            for idx, frame in enumerate(tqdm(frames, desc="ä¿å­˜è§†é¢‘"), start=1):
                writer.append_data(np.array(frame))
                if progress_callback:
                    elapsed = max(time.time() - begin, 0.0)
                    avg_time = elapsed / idx if idx else 0.0
                    progress_callback(min(idx, target_total), target_total, avg_time)
        finally:
            writer.close()

        if progress_callback:
            elapsed = max(time.time() - begin, 0.0)
            avg_time = elapsed / target_total if target_total else 0.0
            progress_callback(target_total, target_total, avg_time)

    def _export_video(
        self,
        output_video,
        output_path: str,
        fps: int,
        total_frames: int,
        start_time: Optional[float],
        progress_callback: Optional[Callable[[int, int, float], None]],
        audio_path: Optional[str],
    ) -> int:
        """Convert the in-memory tensor into a video file."""
        try:
            frames = self._tensor2video(output_video)
            tmp_video_only = str(Path(output_path).with_suffix(".video_only.mp4"))
            self._save_video(
                frames,
                tmp_video_only,
                fps=fps,
                quality=settings.FLASHVSR_EXPORT_VIDEO_QUALITY,
                progress_callback=progress_callback,
                total_frames=total_frames,
                start_time=start_time,
            )
            if audio_path and Path(audio_path).exists():
                self._mux_audio(tmp_video_only, audio_path, output_path)
                Path(tmp_video_only).unlink(missing_ok=True)
            else:
                Path(output_path).parent.mkdir(parents=True, exist_ok=True)
                shutil.move(tmp_video_only, output_path)
            return len(frames)
        finally:
            del output_video

    @staticmethod
    def _should_use_chunk_writer(total_frames: int) -> bool:
        min_frames = settings.FLASHVSR_CHUNKED_SAVE_MIN_FRAMES
        return min_frames > 0 and total_frames >= min_frames

    def _merge_video_chunks(self, chunk_paths: list[Path], output_path: str, audio_path: Optional[str] = None) -> None:
        if not chunk_paths:
            raise RuntimeError("æœªç”Ÿæˆå¯ç”¨äºåˆå¹¶çš„åˆ†ç‰‡")
        chunk_paths.sort(key=lambda path: path.name)
        chunk_dir = chunk_paths[0].parent
        if len(chunk_paths) == 1:
            merged_video = chunk_paths[0]
        else:
            list_file = chunk_dir / f"{Path(output_path).stem}_chunks.txt"
            with open(list_file, "w", encoding="utf-8") as handle:
                for path in chunk_paths:
                    handle.write(f"file '{path}'\n")

            tmp_merged = chunk_dir / f"{Path(output_path).stem}_video_only.mp4"
            cmd = [
                settings.FFMPEG_BINARY,
                "-y",
                "-f",
                "concat",
                "-safe",
                "0",
                "-i",
                str(list_file),
                "-c",
                "copy",
                str(tmp_merged),
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                raise RuntimeError(
                    f"FFmpeg åˆå¹¶åˆ†ç‰‡å¤±è´¥ï¼ˆ{result.returncode}ï¼‰: "
                    f"{result.stderr.strip() or result.stdout.strip()}"
                )

            list_file.unlink(missing_ok=True)
            for path in chunk_paths:
                path.unlink(missing_ok=True)
            merged_video = tmp_merged

        if audio_path and Path(audio_path).exists():
            self._mux_audio(str(merged_video), audio_path, output_path)
            if merged_video != Path(output_path):
                Path(merged_video).unlink(missing_ok=True)
        else:
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            shutil.move(str(merged_video), output_path)
        try:
            chunk_dir.rmdir()
        except OSError:
            pass

    @staticmethod
    def _cleanup_chunk_artifacts(paths: list[Path]) -> None:
        """å°½åŠ›åˆ é™¤å¼‚å¸¸æƒ…å†µä¸‹é—ç•™çš„åˆ†ç‰‡æ–‡ä»¶."""
        for path in paths:
            path.unlink(missing_ok=True)
        if paths:
            try:
                paths[0].parent.rmdir()
            except OSError:
                pass
    @staticmethod
    def _mux_audio(video_path: str, audio_path: str, output_path: str) -> None:
        """Mux existing audio into the given video file."""
        tmp_out = str(Path(output_path).with_suffix(".muxing.tmp.mp4"))
        cmd = [
            settings.FFMPEG_BINARY,
            "-y",
            "-i", video_path,
            "-i", audio_path,
            "-map", "0:v:0",
            "-map", "1:a:0",
            "-c:v", "copy",
            "-c:a", "copy",
            "-shortest",
            tmp_out,
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            # Fallback: transcode audio to AAC
            cmd = [
                settings.FFMPEG_BINARY,
                "-y",
                "-i", video_path,
                "-i", audio_path,
                "-map", "0:v:0",
                "-map", "1:a:0",
                "-c:v", "copy",
                "-c:a", "aac", "-b:a", "192k",
                "-shortest",
                tmp_out,
            ]
            result2 = subprocess.run(cmd, capture_output=True, text=True)
            if result2.returncode != 0:
                raise RuntimeError(
                    f"FFmpeg éŸ³é¢‘åˆå¹¶å¤±è´¥: {result2.stderr.strip() or result2.stdout.strip()}"
                )
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        shutil.move(tmp_out, output_path)
    @staticmethod
    def _largest_8n1_leq(n: int) -> int:
        """è¿”å›æœ€å¤§çš„ 8n+1 <= n."""
        return 0 if n < 1 else ((n - 1) // 8) * 8 + 1

    @staticmethod
    def _estimate_video_bytes(total_frames: int, height: int, width: int, dtype: torch.dtype) -> int:
        element_bytes = torch.finfo(dtype).bits // 8
        return total_frames * height * width * 3 * element_bytes

    def _should_stream_video(self, total_frames: int, height: int, width: int, dtype: torch.dtype) -> bool:
        # Streaming is always enabled as long as prefetch > 0. Use the env limit only to cap buffer size.
        return settings.FLASHVSR_STREAMING_PREFETCH_FRAMES > 0

    def _load_frame_tensor(
        self,
        reader,
        frame_idx: int,
        scale: float,
        target_width: int,
        target_height: int,
        dtype: torch.dtype,
        device: str,
    ) -> torch.Tensor:
        img = Image.fromarray(reader.get_data(frame_idx)).convert('RGB')
        img_out = self._upscale_and_crop(img, scale, target_width, target_height)
        return self._pil_to_tensor(img_out, dtype, device)

    def _frame_array_to_tensor(
        self,
        frame_array,
        scale: float,
        target_width: int,
        target_height: int,
        dtype: torch.dtype,
        device: str,
    ) -> torch.Tensor:
        img = Image.fromarray(frame_array).convert('RGB')
        img_out = self._upscale_and_crop(img, scale, target_width, target_height)
        return self._pil_to_tensor(img_out, dtype, device)

    def _build_streaming_video_tensor(
        self,
        reader,
        indices: list[int],
        scale: float,
        target_width: int,
        target_height: int,
        dtype: torch.dtype,
        target_device: str,
    ) -> StreamingVideoTensor:
        total_needed = len(indices)
        if total_needed == 0:
            raise RuntimeError("è§†é¢‘æ²¡æœ‰å¯å¤„ç†çš„å¸§")
        limit_bytes = settings.FLASHVSR_STREAMING_LQ_MAX_BYTES
        per_frame_bytes = self._estimate_video_bytes(1, target_height, target_width, dtype)
        if per_frame_bytes <= 0:
            raise RuntimeError("æ— æ³•è®¡ç®—å•å¸§ç¼“å†²å¤§å°")
        if limit_bytes <= 0:
            frames_from_limit = total_needed
        else:
            frames_from_limit = limit_bytes // per_frame_bytes
            if frames_from_limit <= 0:
                raise RuntimeError(
                    "FLASHVSR_STREAMING_LQ_MAX_BYTES å¤ªå°ï¼Œè¿å•å¸§ LQ ç¼“å†²éƒ½å®¹çº³ä¸äº†"
                )
        prefetch = max(1, min(settings.FLASHVSR_STREAMING_PREFETCH_FRAMES, total_needed))
        if frames_from_limit < prefetch:
            required_bytes = prefetch * per_frame_bytes
            raise RuntimeError(
                "FLASHVSR_STREAMING_LQ_MAX_BYTES å¤ªå°ï¼Œæ— æ³•é¢„è¯»å¯åŠ¨æ¨ç†æ‰€éœ€çš„å¸§æ•°ï¼›"
                f"è‡³å°‘éœ€è¦ {required_bytes / (1024**3):.2f} GB æ‰èƒ½ç¼“å­˜ {prefetch} å¸§"
            )
        capacity_frames = min(frames_from_limit, total_needed)

        def _read(idx: int):
            return reader.get_data(idx)

        def _process(frame_array) -> torch.Tensor:
            return self._frame_array_to_tensor(
                frame_array,
                scale,
                target_width,
                target_height,
                dtype,
                target_device,
            )

        return StreamingVideoTensor(
            reader=reader,
            indices=list(indices),
            read_frame_fn=_read,
            process_frame_fn=_process,
            height=target_height,
            width=target_width,
            dtype=dtype,
            max_buffer_frames=capacity_frames,
            prefetch_frames=prefetch,
            per_frame_bytes=per_frame_bytes,
            target_device=target_device,
            decode_workers=settings.FLASHVSR_STREAMING_DECODE_THREADS,
            lock_memory=limit_bytes > 0,
        )

    def preload_variant(self, variant: Optional[str] = None) -> PipelineHandle:
        """æ˜¾å¼é¢„åŠ è½½æŒ‡å®šå˜ä½“."""

        normalized = self._normalize_variant(variant)
        return self._get_pipeline_handle(normalized)

    def _get_pipeline_handle(self, variant: str) -> PipelineHandle:
        """è·å–æˆ–åˆå§‹åŒ–æŒ‡å®šå˜ä½“çš„ pipeline."""

        if variant not in self._pipelines:
            with self._lock:
                if variant not in self._pipelines:
                    self._pipelines[variant] = self._build_pipeline_handle(variant)
        return self._pipelines[variant]

    def _build_pipeline_handle(self, variant: str) -> PipelineHandle:
        """æ ¹æ®å˜ä½“åˆå§‹åŒ– pipeline å¹¶ç¼“å­˜.

        å½“å‰å®ç°ä»…æ”¯æŒ tiny_long å˜ä½“ã€‚
        """

        print(f"ğŸš€ åˆå§‹åŒ– FlashVSR {settings.FLASHVSR_VERSION} pipeline ({variant})...")
        model_path = settings.FLASHVSR_MODEL_PATH

        needed_files = list(self.BASE_MODEL_FILES)

        missing = [name for name in needed_files if not (model_path / name).exists()]
        if missing:
            raise FileNotFoundError(
                "ç¼ºå°‘ FlashVSR æƒé‡æ–‡ä»¶: " + ", ".join(missing) + f" (æ ¹ç›®å½•: {model_path})"
            )

        mm = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        weights_to_load = [str(model_path / self.BASE_MODEL_FILES[0])]
        mm.load_models(weights_to_load)

        if not self.PROMPT_TENSOR_FILE.exists():
            raise FileNotFoundError(
                "ç¼ºå°‘ FlashVSR prompt tensor: "
                f"{self.PROMPT_TENSOR_FILE}. è¯·å°† posi_prompt.pth "
                "æ”¾ç½®åœ¨ models/FlashVSR-v1.1/ ä¸‹æˆ–é€šè¿‡ FLASHVSR_PROMPT_TENSOR_PATH è¦†ç›–ï¼Œè¯¦è§ docs/deployment.mdã€‚"
            )

        prompt_tensor = torch.load(self.PROMPT_TENSOR_FILE, map_location="cpu")

        pipeline_cls = FlashVSRTinyLongPipeline

        device = self._resolve_device()
        print(f"ğŸ“ ä½¿ç”¨è®¾å¤‡: {device}")
        if device.startswith("cuda"):
            gpu_index = 0
            try:
                if ":" in device:
                    gpu_index = int(device.split(":", 1)[1])
            except Exception:
                gpu_index = 0
            try:
                print(f"ğŸ® GPU: {torch.cuda.get_device_name(gpu_index)}")
            except Exception:
                pass

        pipe = pipeline_cls.from_model_manager(mm, device=device)

        cache_device, cache_reason = self._decide_cache_offload_device(device)
        pipe.set_cache_offload_device(cache_device)
        if cache_device:
            print(f"ğŸ’¾ KV cache offload â†’ {cache_device} ({cache_reason})")

        # é…ç½® LQ æŠ•å½±å±‚
        lq_proj = Causal_LQ4x_Proj(in_dim=3, out_dim=1536, layer_num=1).to(
            device, dtype=torch.bfloat16
        )
        lq_proj.load_state_dict(
            torch.load(model_path / "LQ_proj_in.ckpt", map_location="cpu"),
            strict=True,
        )
        lq_proj.to(device)
        pipe.denoising_model().LQ_proj_in = lq_proj

        # é…ç½® TCDecoder
        multi_scale_channels = [512, 256, 128, 128]
        pipe.TCDecoder = build_tcdecoder(
            new_channels=multi_scale_channels, new_latent_channels=16 + 768
        )
        pipe.TCDecoder.load_state_dict(
            torch.load(model_path / "TCDecoder.ckpt", map_location="cpu"),
            strict=False,
        )

        # å¯é€‰ï¼šæµæ°´çº¿å¹¶è¡Œï¼ˆå¤š GPUï¼‰
        pp_devices, pp_split = self._parse_pipeline_parallel()

        default_kwargs: dict[str, Any] = {}

        pipe.to(device)
        # å¯ç”¨æµæ°´çº¿å¹¶è¡Œæ—¶ï¼Œä¸å¯ç”¨ VRAM management é¿å…è®¾å¤‡é”™é…
        if pp_devices is None:
            pipe.enable_vram_management(num_persistent_param_in_dit=None)
        pipe.init_cross_kv(context_tensor=prompt_tensor)
        pipe.load_models_to_device(["dit", "vae"])

        # åˆå§‹åŒ–æµæ°´çº¿å¹¶è¡Œï¼ˆéœ€è¦åœ¨ init_cross_kv ä¹‹åï¼ŒæŠŠ cross-attn ç¼“å­˜ä¹Ÿè¿ç§»ï¼‰
        if pp_devices is not None and hasattr(pipe, "enable_pipeline_parallel"):
            try:
                pipe.enable_pipeline_parallel(pp_devices, split_index=pp_split)
                print(f"ğŸ”€ Pipeline parallel enabled on {pp_devices} (split @ block {pp_split if pp_split is not None else 'auto'})")
            except Exception as e:
                print(f"âš ï¸ å¯ç”¨æµæ°´çº¿å¹¶è¡Œå¤±è´¥ï¼š{e}")
            # When PP is enabled, move TCDecoder to the last stage device to free GPU0 for Stage0
            try:
                dev1 = pp_devices[-1]
                if hasattr(pipe, "TCDecoder") and pipe.TCDecoder is not None:
                    pipe.TCDecoder.to(dev1)
                    print(f"ğŸ¯ TCDecoder moved to {dev1} for overlap")
            except Exception as e:
                print(f"âš ï¸ TCDecoder è¿ç§»åˆ° {pp_devices[-1]} å¤±è´¥ï¼š{e}")
        # Overlap scheduling for single video (optional)
        try:
            if getattr(settings, "FLASHVSR_PP_OVERLAP", False) and hasattr(pipe, "enable_pipeline_overlap"):
                pipe.enable_pipeline_overlap(True)
                print("â© Pipeline overlap (Stage0/Stage1) enabled per window")
        except Exception as e:
            print(f"âš ï¸ å¯ç”¨æµæ°´çº¿é‡å å¤±è´¥ï¼š{e}")

        print(f"âœ… FlashVSR pipeline ({variant}) åˆå§‹åŒ–å®Œæˆ")
        return PipelineHandle(
            variant=variant,
            pipeline=pipe,
            device=device,
            default_kwargs=default_kwargs,
        )

    def _normalize_variant(self, variant: Optional[str]) -> str:
        value = (variant or settings.DEFAULT_MODEL_VARIANT).lower()
        if value not in self.SUPPORTED_VARIANTS:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ¨¡å‹å˜ä½“: {variant}. å¯é€‰: {', '.join(self.SUPPORTED_VARIANTS)}"
            )
        asset_status = self.inspect_assets().get("ready_variants", {})
        if not asset_status.get(value, False):
            raise RuntimeError(
                f"æ¨¡å‹å˜ä½“ {value} ç¼ºå°‘å¿…è¦æƒé‡ï¼Œè¯·å‚è€ƒ README ä¸‹è½½ FlashVSR {settings.FLASHVSR_VERSION} æƒé‡"
            )
        return value

    def _decide_cache_offload_device(self, device: str) -> tuple[Optional[str], Optional[str]]:
        """
        Determine whether to spill streaming KV caches to CPU, returning (device, reason).
        """
        mode = (settings.FLASHVSR_CACHE_OFFLOAD or "auto").strip().lower()
        allowed = {"auto", "cpu", "none", "off", "disable"}
        if mode not in allowed:
            raise ValueError(
                f"æ— æ•ˆçš„ FLASHVSR_CACHE_OFFLOAD é…ç½®: {settings.FLASHVSR_CACHE_OFFLOAD}. "
                f"å¯é€‰å€¼: {', '.join(sorted(allowed))}"
            )
        if not device.startswith("cuda"):
            return None, None

        # Query the correct GPU properties if a specific index is requested
        gpu_index = 0
        try:
            if ":" in device:
                gpu_index = int(device.split(":", 1)[1])
        except Exception:
            gpu_index = 0
        total_gb = torch.cuda.get_device_properties(gpu_index).total_memory / (1024 ** 3)
        threshold = settings.FLASHVSR_CACHE_OFFLOAD_AUTO_THRESHOLD_GB

        if mode == "cpu":
            return "cpu", "forced via FLASHVSR_CACHE_OFFLOAD=cpu"
        if mode == "auto" and total_gb <= threshold:
            return (
                "cpu",
                f"auto: GPU {total_gb:.1f} GB â‰¤ {threshold:.1f} GB",
            )
        return None, None

    def _resolve_device(self) -> str:
        """Resolve target torch device from settings and availability."""
        override = (settings.FLASHVSR_DEVICE or "").strip()
        if override:
            if override.startswith("cuda"):
                if torch.cuda.is_available():
                    # Optionally set current device if index provided
                    try:
                        if ":" in override:
                            idx = int(override.split(":", 1)[1])
                            torch.cuda.set_device(idx)
                    except Exception:
                        pass
                    return override
                return "cpu"
            if override == "cpu":
                return "cpu"
        return "cuda" if torch.cuda.is_available() else "cpu"

    def _parse_pipeline_parallel(self) -> tuple[Optional[list[str]], Optional[int]]:
        """Parse pipeline-parallel settings from env Settings.
        Returns (devices, split_index) or (None, None) if disabled.
        """
        raw = (settings.FLASHVSR_PP_DEVICES or "").strip()
        if not raw:
            return None, None
        parts = [p.strip() for p in raw.split(",") if p.strip()]
        devices: list[str] = []
        for p in parts:
            if p.startswith("cuda"):
                devices.append(p)
            elif p.isdigit():
                devices.append(f"cuda:{p}")
            else:
                # fallback: accept 'cpu' or unknown
                devices.append(p)
        # Need at least 2 devices
        if len(devices) < 2:
            return None, None

        split_raw = (settings.FLASHVSR_PP_SPLIT_BLOCK or "auto").strip().lower()
        split_index: Optional[int]
        if split_raw in ("", "auto"):
            split_index = None
        else:
            try:
                split_index = int(split_raw)
            except Exception:
                split_index = None
        return devices, split_index
