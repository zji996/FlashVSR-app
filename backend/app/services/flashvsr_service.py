"""FlashVSR æŽ¨ç†æœåŠ¡å°è£…."""

import os
import sys
import time
from pathlib import Path
from typing import Optional, Callable

import torch
import numpy as np
from PIL import Image
import imageio
from tqdm import tqdm
from einops import rearrange

from app.config import settings

# æ·»åŠ FlashVSRåˆ°Pythonè·¯å¾„
THIRD_PARTY_ROOT = Path("/app/third_party")
FLASHVSR_PATH = THIRD_PARTY_ROOT / "FlashVSR"
if str(FLASHVSR_PATH) not in sys.path:
    sys.path.insert(0, str(FLASHVSR_PATH))

from diffsynth import ModelManager, FlashVSRTinyPipeline

# å¯¼å…¥FlashVSRå·¥å…·å‡½æ•°
WANVSR_PATH = FLASHVSR_PATH / "examples" / "WanVSR"
if str(WANVSR_PATH) not in sys.path:
    sys.path.insert(0, str(WANVSR_PATH))

from utils.utils import Buffer_LQ4x_Proj
from utils.TCDecoder import build_tcdecoder


class FlashVSRService:
    """FlashVSRæŽ¨ç†æœåŠ¡ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰."""
    
    _instance: Optional['FlashVSRService'] = None
    _pipeline = None
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼."""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–."""
        if self._pipeline is None:
            self._init_pipeline()
    
    def _init_pipeline(self):
        """åˆå§‹åŒ–FlashVSR pipeline."""
        print("ðŸš€ æ­£åœ¨åˆå§‹åŒ– FlashVSR pipeline...")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ðŸ“ ä½¿ç”¨è®¾å¤‡: {device}")
        
        if device == "cuda":
            print(f"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}")
        
        # åŠ è½½æ¨¡åž‹
        mm = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        model_path = settings.FLASHVSR_MODEL_PATH

        mm.load_models(
            [
                str(model_path / "diffusion_pytorch_model_streaming_dmd.safetensors"),
            ]
        )
        
        self._pipeline = FlashVSRTinyPipeline.from_model_manager(mm, device=device)
        
        # åŠ è½½LQæŠ•å½±å±‚
        self._pipeline.denoising_model().LQ_proj_in = Buffer_LQ4x_Proj(
            in_dim=3, out_dim=1536, layer_num=1
        ).to(device, dtype=torch.bfloat16)
        
        lq_proj_path = model_path / "LQ_proj_in.ckpt"
        if lq_proj_path.exists():
            self._pipeline.denoising_model().LQ_proj_in.load_state_dict(
                torch.load(lq_proj_path, map_location="cpu"), strict=True
            )
        self._pipeline.denoising_model().LQ_proj_in.to(device)
        
        # åŠ è½½TCDecoder
        multi_scale_channels = [512, 256, 128, 128]
        self._pipeline.TCDecoder = build_tcdecoder(
            new_channels=multi_scale_channels, new_latent_channels=16 + 768
        )
        self._pipeline.TCDecoder.load_state_dict(
            torch.load(model_path / "TCDecoder.ckpt"), strict=False
        )
        
        self._pipeline.to(device)
        self._pipeline.enable_vram_management(num_persistent_param_in_dit=None)
        self._pipeline.init_cross_kv()
        self._pipeline.load_models_to_device(["dit", "vae"])
        
        print("âœ… FlashVSR pipeline åˆå§‹åŒ–å®Œæˆ")
    
    def process_video(
        self,
        input_path: str,
        output_path: str,
        scale: float = 4.0,
        sparse_ratio: float = 2.0,
        local_range: int = 11,
        seed: int = 0,
        progress_callback: Optional[Callable[[int, int, float], None]] = None,
    ) -> dict:
        """
        å¤„ç†è§†é¢‘è¶…åˆ†è¾¨çŽ‡.
        
        Args:
            input_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            scale: è¶…åˆ†å€æ•°
            sparse_ratio: ç¨€ç–æ¯”çŽ‡
            local_range: å±€éƒ¨èŒƒå›´
            seed: éšæœºç§å­
            progress_callback: è¿›åº¦å›žè°ƒå‡½æ•°(processed_frames, total_frames, avg_time)
        
        Returns:
            åŒ…å«è§†é¢‘ä¿¡æ¯çš„å­—å…¸
        """
        print(f"ðŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘: {input_path}")
        start_time = time.time()
        
        # å‡†å¤‡è¾“å…¥
        video_tensor, height, width, total_frames, fps = self._prepare_input(
            input_path, scale
        )
        
        print(f"ðŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {total_frames}å¸§, {fps}fps")
        
        # è¶…åˆ†å¤„ç†
        device = "cuda" if torch.cuda.is_available() else "cpu"

        if progress_callback and total_frames:
            progress_callback(0, total_frames, 0.0)

        # å¤„ç†è§†é¢‘
        infer_start = time.time()
        output_video = self._pipeline(
            prompt="",
            negative_prompt="",
            cfg_scale=1.0,
            num_inference_steps=1,
            seed=seed,
            LQ_video=video_tensor,
            num_frames=total_frames,
            height=height,
            width=width,
            is_full_block=False,
            if_buffer=True,
            topk_ratio=sparse_ratio * 768 * 1280 / (height * width),
            kv_ratio=3.0,
            local_range=local_range,
            color_fix=True,
        )
        inference_time = time.time() - infer_start
        
        # è½¬æ¢ä¸ºè§†é¢‘å¸§
        frames = self._tensor2video(output_video)
        
        # ä¿å­˜è§†é¢‘
        self._save_video(
            frames,
            output_path,
            fps=fps,
            progress_callback=progress_callback,
            total_frames=total_frames,
            start_time=start_time,
        )
        
        total_time = time.time() - start_time
        
        print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {output_path}")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # æ¸…ç†GPUç¼“å­˜
        if device == "cuda":
            torch.cuda.empty_cache()
        
        return {
            "width": width,
            "height": height,
            "total_frames": total_frames,
            "fps": fps,
            "processed_frames": len(frames),
            "inference_time": inference_time,
            "processing_time": total_time,
        }
    
    def _prepare_input(self, path: str, scale: float):
        """å‡†å¤‡è¾“å…¥è§†é¢‘tensor."""
        device = "cuda" if torch.cuda.is_available() else "cpu"
        dtype = torch.bfloat16
        
        # è¯»å–è§†é¢‘
        reader = imageio.get_reader(path)
        first_frame = Image.fromarray(reader.get_data(0)).convert('RGB')
        w0, h0 = first_frame.size
        
        # èŽ·å–å…ƒæ•°æ®
        meta = {}
        try:
            meta = reader.get_meta_data()
        except:
            pass
        
        fps_val = meta.get('fps', 30)
        fps = int(round(fps_val)) if isinstance(fps_val, (int, float)) else 30
        
        # èŽ·å–æ€»å¸§æ•°
        total_frames = self._count_frames(reader, meta)
        
        print(f"åŽŸå§‹åˆ†è¾¨çŽ‡: {w0}x{h0}, åŽŸå§‹å¸§æ•°: {total_frames}, FPS: {fps}")
        
        # è®¡ç®—ç›®æ ‡å°ºå¯¸
        sW, sH, tW, tH = self._compute_scaled_dims(w0, h0, scale)
        print(f"ç›®æ ‡åˆ†è¾¨çŽ‡: {tW}x{tH} (ç¼©æ”¾ {scale}x)")
        
        # è¯»å–æ‰€æœ‰å¸§
        frames = []
        indices = list(range(total_frames)) + [total_frames - 1] * 4
        F = self._largest_8n1_leq(len(indices))
        indices = indices[:F]
        
        print(f"å¤„ç†å¸§æ•°: {F}")
        
        try:
            for i in tqdm(indices, desc="åŠ è½½è§†é¢‘å¸§"):
                img = Image.fromarray(reader.get_data(i)).convert('RGB')
                img_out = self._upscale_and_crop(img, scale, tW, tH)
                frames.append(self._pil_to_tensor(img_out, dtype, device))
        finally:
            reader.close()
        
        video_tensor = torch.stack(frames, 0).permute(1, 0, 2, 3).unsqueeze(0)
        return video_tensor, tH, tW, F, fps
    
    @staticmethod
    def _count_frames(reader, meta):
        """è®¡ç®—è§†é¢‘æ€»å¸§æ•°."""
        try:
            nf = meta.get('nframes', None)
            if isinstance(nf, int) and nf > 0:
                return nf
        except:
            pass
        
        try:
            return reader.count_frames()
        except:
            n = 0
            try:
                while True:
                    reader.get_data(n)
                    n += 1
            except:
                return n
    
    @staticmethod
    def _compute_scaled_dims(w0: int, h0: int, scale: float, multiple: int = 128):
        """è®¡ç®—ç¼©æ”¾åŽçš„å°ºå¯¸."""
        sW = int(round(w0 * scale))
        sH = int(round(h0 * scale))
        
        tW = (sW // multiple) * multiple
        tH = (sH // multiple) * multiple
        
        return sW, sH, tW, tH
    
    @staticmethod
    def _upscale_and_crop(img: Image.Image, scale: float, tW: int, tH: int):
        """æ”¾å¤§å¹¶å±…ä¸­è£å‰ª."""
        w0, h0 = img.size
        sW = int(round(w0 * scale))
        sH = int(round(h0 * scale))
        
        up = img.resize((sW, sH), Image.BICUBIC)
        l = (sW - tW) // 2
        t = (sH - tH) // 2
        return up.crop((l, t, l + tW, t + tH))
    
    @staticmethod
    def _pil_to_tensor(img: Image.Image, dtype, device):
        """PILå›¾åƒè½¬tensor."""
        t = torch.from_numpy(np.asarray(img, np.uint8)).to(
            device=device, dtype=torch.float32
        )
        t = t.permute(2, 0, 1) / 255.0 * 2.0 - 1.0
        return t.to(dtype)
    
    @staticmethod
    def _tensor2video(frames):
        """Tensorè½¬è§†é¢‘å¸§."""
        frames = rearrange(frames, "C T H W -> T H W C")
        frames = ((frames.float() + 1) * 127.5).clip(0, 255).cpu().numpy().astype(np.uint8)
        return [Image.fromarray(frame) for frame in frames]
    
    @staticmethod
    def _save_video(
        frames,
        save_path: str,
        fps: int = 30,
        quality: int = 6,
        progress_callback: Optional[Callable[[int, int, float], None]] = None,
        total_frames: Optional[int] = None,
        start_time: Optional[float] = None,
    ):
        """ä¿å­˜è§†é¢‘."""
        target_total = total_frames or len(frames)
        begin = start_time or time.time()

        save_dir = Path(save_path).parent
        save_dir.mkdir(parents=True, exist_ok=True)

        writer = imageio.get_writer(save_path, fps=fps, quality=quality)
        try:
            for idx, frame in enumerate(tqdm(frames, desc="ä¿å­˜è§†é¢‘"), start=1):
                writer.append_data(np.array(frame))
                if progress_callback:
                    elapsed = max(time.time() - begin, 0.0)
                    avg_time = elapsed / idx if idx else 0.0
                    progress_callback(min(idx, target_total), target_total, avg_time)
        finally:
            writer.close()

        if progress_callback:
            elapsed = max(time.time() - begin, 0.0)
            avg_time = elapsed / target_total if target_total else 0.0
            progress_callback(target_total, target_total, avg_time)
    
    @staticmethod
    def _largest_8n1_leq(n: int) -> int:
        """è¿”å›žæœ€å¤§çš„ 8n+1 <= n."""
        return 0 if n < 1 else ((n - 1) // 8) * 8 + 1
